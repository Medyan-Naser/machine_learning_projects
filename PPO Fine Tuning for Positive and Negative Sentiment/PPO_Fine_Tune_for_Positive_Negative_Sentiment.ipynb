{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user torch==2.3.1 torchtext==0.17.2\n",
    "!pip install --user datasets==3.2.0\n",
    "!pip install --user trl==0.11\n",
    "!pip install --user transformers==4.43.4\n",
    "!pip install --user nltk==3.9.1 rouge_score==0.1.2\n",
    "!pip install --user matplotlib==3.10.0 \n",
    "!pip install numpy==1.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e64a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer,AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "import os\n",
    "\n",
    "import tarfile\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Disable warnings for a cleaner notebook or console experience\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842df524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save a dictionary to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The dictionary to save.\n",
    "        file_path (str): The path to the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"Data successfully saved to {file_path}\")\n",
    "    \n",
    "    \n",
    "def load_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The data loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8365fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_to_length(tensor, length, pad_token_id):\n",
    "    padding_length = length - tensor.size(0)\n",
    "    if padding_length > 0:\n",
    "        padding = torch.full((padding_length,), pad_token_id, dtype=torch.long, device=tensor.device)\n",
    "        return torch.cat((tensor, padding))\n",
    "    return tensor\n",
    "\n",
    "def pad_list_to_batch_size(tensors, batch_size, pad_token_id):\n",
    "    max_length = max(t.size(0) for t in tensors)\n",
    "    padded_tensors = [pad_sequence_to_length(t, max_length, pad_token_id) for t in tensors]\n",
    "\n",
    "    # Add additional padding-only tensors if needed\n",
    "    while len(padded_tensors) < batch_size:\n",
    "        padded_tensors.append(torch.full((max_length,), pad_token_id, dtype=torch.long, device=tensors[0].device))\n",
    "\n",
    "    return padded_tensors[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c23e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ppo_stats(stats, related_to_objective=False):\n",
    "    print(\"PPO Training Statistics\\n\")\n",
    "\n",
    "    if related_to_objective:\n",
    "        print(\"Objective Statistics:\")\n",
    "        print(f\"  KL Divergence (objective/kl): {stats['objective/kl']}\")\n",
    "        print(f\"  KL Coefficient (objective/kl_coef): {stats['objective/kl_coef']}\")\n",
    "        print(f\"  Entropy (objective/entropy): {stats['objective/entropy']}\\n\")\n",
    "        \n",
    "        print(\"PPO Losses (Related to Minimizing Objective Function):\")\n",
    "        print(f\"  Policy Loss (ppo/loss/policy): {stats['ppo/loss/policy']}\")\n",
    "        print(f\"  Value Loss (ppo/loss/value): {stats['ppo/loss/value']}\")\n",
    "        print(f\"  Total Loss (ppo/loss/total): {stats['ppo/loss/total']}\\n\")\n",
    "        \n",
    "        print(\"PPO Policy Statistics:\")\n",
    "        print(f\"  Policy Entropy (ppo/policy/entropy): {stats['ppo/policy/entropy']}\")\n",
    "        print(f\"  Approx KL (ppo/policy/approxkl): {stats['ppo/policy/approxkl']}\")\n",
    "        print(f\"  Clip Fraction (ppo/policy/clipfrac): {stats['ppo/policy/clipfrac']}\\n\")\n",
    "    else:\n",
    "        print(\"Reward and Value Function Estimation:\")\n",
    "        print(f\"  Mean Non-Score Reward (ppo/mean_non_score_reward): {stats['ppo/mean_non_score_reward']}\")\n",
    "        print(f\"  Mean Scores (ppo/mean_scores): {stats['ppo/mean_scores']}\")\n",
    "        print(f\"  Std Scores (ppo/std_scores): {stats['ppo/std_scores']}\")\n",
    "        print(f\"  Value Prediction (ppo/val/vpred): {stats['ppo/val/vpred']}\")\n",
    "        print(f\"  Value Prediction Error (ppo/val/error): {stats['ppo/val/error']}\")\n",
    "        print(f\"  Value Prediction Variance (ppo/val/var): {stats['ppo/val/var']}\")\n",
    "        print(f\"  Value Prediction Mean (ppo/val/mean): {stats['ppo/val/mean']}\")\n",
    "        print(f\"  Explained Variance (ppo/val/var_explained): {stats['ppo/val/var_explained']}\\n\")\n",
    "    \n",
    "    print(\"Token Lengths:\")\n",
    "    print(f\"  Queries Length Mean (tokens/queries_len_mean): {stats['tokens/queries_len_mean']}\")\n",
    "    print(f\"  Responses Length Mean (tokens/responses_len_mean): {stats['tokens/responses_len_mean']}\\n\")\n",
    "    \n",
    "    print(\"Time Statistics:\")\n",
    "    print(f\"  Total Time (time/ppo/total): {stats['time/ppo/total']} seconds\\n\")\n",
    "\n",
    "# Example usage with the provided stats and the flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba38231",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"lvwerra/gpt2-imdb\",\n",
    "    learning_rate=1.41e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d572883",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_kwargs = {\"top_k\":None, \"function_to_apply\": \"none\", \"batch_size\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d943b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first model\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21192251",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"imdb\"\n",
    "ds = load_dataset(dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "for sample in range(N):\n",
    "    print('text',ds[sample]['text'])\n",
    "    print('label',ds[sample]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_min_text_length, input_max_text_length = 2, 8\n",
    "input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "input_size\n",
    "sample=ds[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "sample[\"input_ids\"]\n",
    "sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "sample[\"query\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "    sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dde7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(tokenize, batched=False)\n",
    "ds.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64490cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(ds):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Review: {sample['review']}\")\n",
    "    print(f\"Input IDs: {sample['input_ids']}\")\n",
    "    print(f\"Query: {sample['query']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
