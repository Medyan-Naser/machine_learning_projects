# Instruction Tuning for LLMs

## What is Instruction Tuning?
Instruction tuning, also known as **Supervised Fine-Tuning (SFT)**, is the process of adapting a pretrained language model to follow natural language instructions.  
It uses curated datasets of **instruction–response pairs**, teaching the model to perform tasks more reliably and in a user-friendly way.  

This step is typically applied **after pretraining** and before advanced alignment methods like **RLHF** (Reinforcement Learning with Human Feedback) or **DPO** (Direct Preference Optimization).

---

## Role in the Training Pipeline
- **Pretraining** – Model learns general language patterns by predicting the next token.  
- **Instruction Tuning** – Model learns to follow explicit instructions with supervised examples.  
- **RLHF / DPO** – Model is further aligned with human preferences.  

Instruction tuning provides the **foundation** that makes RLHF or DPO effective.  

---

## Dataset Structure
Instruction tuning relies on structured templates with up to three components:

- **Instruction** – Describes the task (e.g., *“Summarize this paragraph”*).  
- **Input** – Optional context or data the instruction refers to.  
- **Output** – The expected response generated by the model.  

Some datasets only use *instruction + output* pairs without input.

---

## Prompt Formatting
Special tokens or delimiters are used to clearly separate sections of a prompt, ensuring the tokenizer interprets them correctly.  
Common formats include:  

Instruction:
Translate to French
Input:
Hello world
Response:
Bonjour le monde


Different models may use different tokens (e.g., *Human/Assistant*). Consistency across the dataset is critical.  

---

## Instruction Masking
By default, language models learn from every token they predict.  
Instruction tuning often uses **masking** so the loss is applied only to the **response tokens**, not to the instruction itself.  

- Instruction masking enhances transformer-based model fine-tuning by ensuring loss computation focuses only on response tokens, improving accuracy and contextual relevance. This technique prevents models from overfitting to instruction patterns, optimizes computational efficiency, and enhances user interactions in AI-driven applications such as chatbots and Q&A systems. 

**Why it matters**:  
- Focuses learning on the target output.  
- Reduces wasted computation on repeating instructions.  
- Improves response quality for task-oriented fine-tuning.  

Frameworks like Hugging Face provide utilities (e.g., `DataCollatorForCompletionOnlyLM`) to apply masking automatically.  

---

## Reward Modeling and Response Evaluation

Reward modeling introduces **human preferences** into the training process by assigning scalar rewards to model responses.  
It is a key step in **RLHF (Reinforcement Learning with Human Feedback)**, ensuring models produce reliable, factual, and preference-aligned outputs.  

### Core Functions
- **Alignment** – Evaluates how well responses match human intent.  
- **Quality Scoring** – Assigns numerical values to responses for comparison.  
- **Optimization Guidance** – Adjusts model parameters to maximize reward scores.  
- **Preference Integration** – Customizes behavior to user needs (e.g., factual vs. creative).  
- **Consistency** – Provides stable and repeatable evaluation across queries.  

### Response Evaluation
Responses are scored against human preferences:
- Correct and factual answers receive **high scores**.  
- Irrelevant or incorrect answers receive **low scores**.  
This process helps prioritize **accuracy and contextual relevance**.  

---

### Types of Reward Modeling
Reward modeling can take different forms depending on the supervision:

- **Pairwise comparison (most common):**  
  The model learns which response is better by comparing pairs (chosen vs. rejected).  
- **Pointwise scoring:**  
  Each response is rated independently with a quality score.  
- **Listwise ranking:**  
  Multiple responses are ranked at once (less common, but more expressive).  

Pairwise comparison is preferred because humans find it easier to say *“A is better than B”* rather than giving absolute numeric scores.

---

## Training Workflow
Reward models are typically trained with datasets such as **Dahoas/synthetic-instruction-gptj-pairwise** from Hugging Face, which provide prompts, a preferred response (*chosen*), and a rejected response.  

### Steps
1. **Dataset preparation:**  
    - Prompts are combined with chosen and rejected responses.  
    - Both are tokenized (converted into model-readable format).  
    - Length filtering ensures inputs stay within model limits.  

2. **Scoring function setup:**  
    - A base model (e.g., GPT-2 for sequence classification) is adapted to output a **single scalar score**.  
    - The final layer has **one neuron** that predicts response quality.  

3. **Reward loss (conceptual):**  
    - The model is trained to give higher scores to chosen responses than rejected ones.  
    - Loss functions enforce a larger gap between good and bad scores.  

4. **Evaluation:**  
    - Performance is measured by **win rate**: how often the reward model prefers the correct (chosen) response.  
    - Top models reach 60–70% win rates on real datasets.  

---

## Using LoRA with Reward Models
Reward modeling can be computationally heavy because it requires fine-tuning large models.  
This is where **LoRA (Low-Rank Adaptation)** is useful:

- Instead of updating all model weights, LoRA inserts small trainable matrices into the model.  
- This reduces the number of trainable parameters, lowers memory usage, and speeds up training.  
- LoRA can be applied to the **reward model itself** during training, making preference learning more efficient.  

Example usage:
- Apply LoRA to the classification head and attention layers of the reward model.  
- Configure training with libraries like **PEFT (Parameter-Efficient Fine-Tuning)** and Hugging Face’s `RewardTrainer`.  

---

### Reward Modeling with Other Tuning Techniques
Reward models don’t exist in isolation—they work alongside other fine-tuning methods:

- **Instruction Tuning:**  
  Teaches the model to follow natural language instructions. Reward modeling can refine instruction tuning by ranking which outputs follow instructions *best*.  

- **RLHF (Reinforcement Learning with Human Feedback):**  
  The reward model is used inside RLHF as the “critic” that guides the policy model.  

- **Direct Preference Optimization (DPO):**  
  A newer approach that skips reinforcement learning and directly optimizes on human preferences. Reward modeling concepts still apply here, but training is simplified.  

- **LoRA + Reward Modeling:**  
  Efficiently trains large reward models by reducing compute requirements. This makes RLHF and preference optimization more practical on limited hardware.  

---

### Why Reward Modeling Matters
Reward models are crucial because they:
- Translate **human feedback** into machine-readable signals.  
- Improve **alignment** between LLMs and user intent.  
- Enable efficient use of tuning methods like **LoRA, RLHF, and DPO**.  
- Provide a scalable way to optimize for **factuality, safety, and usefulness**.  