# Instruction Tuning for LLMs

## What is Instruction Tuning?
Instruction tuning, also known as **Supervised Fine-Tuning (SFT)**, is the process of adapting a pretrained language model to follow natural language instructions.  
It uses curated datasets of **instruction–response pairs**, teaching the model to perform tasks more reliably and in a user-friendly way.  

This step is typically applied **after pretraining** and before advanced alignment methods like **RLHF** (Reinforcement Learning with Human Feedback) or **DPO** (Direct Preference Optimization).

---

## Role in the Training Pipeline
- **Pretraining** – Model learns general language patterns by predicting the next token.  
- **Instruction Tuning** – Model learns to follow explicit instructions with supervised examples.  
- **RLHF / DPO** – Model is further aligned with human preferences.  

Instruction tuning provides the **foundation** that makes RLHF or DPO effective.  

---

## Dataset Structure
Instruction tuning relies on structured templates with up to three components:

- **Instruction** – Describes the task (e.g., *“Summarize this paragraph”*).  
- **Input** – Optional context or data the instruction refers to.  
- **Output** – The expected response generated by the model.  

Some datasets only use *instruction + output* pairs without input.

---

## Prompt Formatting
Special tokens or delimiters are used to clearly separate sections of a prompt, ensuring the tokenizer interprets them correctly.  
Common formats include:  

Instruction:
Translate to French
Input:
Hello world
Response:
Bonjour le monde


Different models may use different tokens (e.g., *Human/Assistant*). Consistency across the dataset is critical.  

---

## Instruction Masking
By default, language models learn from every token they predict.  
Instruction tuning often uses **masking** so the loss is applied only to the **response tokens**, not to the instruction itself.  

- Instruction masking enhances transformer-based model fine-tuning by ensuring loss computation focuses only on response tokens, improving accuracy and contextual relevance. This technique prevents models from overfitting to instruction patterns, optimizes computational efficiency, and enhances user interactions in AI-driven applications such as chatbots and Q&A systems. 

**Why it matters**:  
- Focuses learning on the target output.  
- Reduces wasted computation on repeating instructions.  
- Improves response quality for task-oriented fine-tuning.  

Frameworks like Hugging Face provide utilities (e.g., `DataCollatorForCompletionOnlyLM`) to apply masking automatically.  

---
