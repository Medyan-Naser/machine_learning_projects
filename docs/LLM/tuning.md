# Instruction Tuning for LLMs

## What is Instruction Tuning?
Instruction tuning, also known as **Supervised Fine-Tuning (SFT)**, is the process of adapting a pretrained language model to follow natural language instructions.  
It uses curated datasets of **instruction–response pairs**, teaching the model to perform tasks more reliably and in a user-friendly way.  

This step is typically applied **after pretraining** and before advanced alignment methods like **RLHF** (Reinforcement Learning with Human Feedback) or **DPO** (Direct Preference Optimization).

---

## Role in the Training Pipeline
- **Pretraining** – Model learns general language patterns by predicting the next token.  
- **Instruction Tuning** – Model learns to follow explicit instructions with supervised examples.  
- **RLHF / DPO** – Model is further aligned with human preferences.  

Instruction tuning provides the **foundation** that makes RLHF or DPO effective.  

---

## Dataset Structure
Instruction tuning relies on structured templates with up to three components:

- **Instruction** – Describes the task (e.g., *“Summarize this paragraph”*).  
- **Input** – Optional context or data the instruction refers to.  
- **Output** – The expected response generated by the model.  

Some datasets only use *instruction + output* pairs without input.

---

## Prompt Formatting
Special tokens or delimiters are used to clearly separate sections of a prompt, ensuring the tokenizer interprets them correctly.  
Common formats include:  

Instruction:
Translate to French
Input:
Hello world
Response:
Bonjour le monde


Different models may use different tokens (e.g., *Human/Assistant*). Consistency across the dataset is critical.  

---

## Instruction Masking
By default, language models learn from every token they predict.  
Instruction tuning often uses **masking** so the loss is applied only to the **response tokens**, not to the instruction itself.  

- Instruction masking enhances transformer-based model fine-tuning by ensuring loss computation focuses only on response tokens, improving accuracy and contextual relevance. This technique prevents models from overfitting to instruction patterns, optimizes computational efficiency, and enhances user interactions in AI-driven applications such as chatbots and Q&A systems. 

**Why it matters**:  
- Focuses learning on the target output.  
- Reduces wasted computation on repeating instructions.  
- Improves response quality for task-oriented fine-tuning.  

Frameworks like Hugging Face provide utilities (e.g., `DataCollatorForCompletionOnlyLM`) to apply masking automatically.  

---

## Reward Modeling and Response Evaluation

Reward modeling introduces **human preferences** into the training process by assigning scalar rewards to model responses.  
It is a key step in **RLHF (Reinforcement Learning with Human Feedback)**, ensuring models produce reliable, factual, and preference-aligned outputs.  

### Core Functions
- **Alignment** – Evaluates how well responses match human intent.  
- **Quality Scoring** – Assigns numerical values to responses for comparison.  
- **Optimization Guidance** – Adjusts model parameters to maximize reward scores.  
- **Preference Integration** – Customizes behavior to user needs (e.g., factual vs. creative).  
- **Consistency** – Provides stable and repeatable evaluation across queries.  

### Response Evaluation
Responses are scored against human preferences:
- Correct and factual answers receive **high scores**.  
- Irrelevant or incorrect answers receive **low scores**.  
This process helps prioritize **accuracy and contextual relevance**.  

---

### Types of Reward Modeling
Reward modeling can take different forms depending on the supervision:

- **Pairwise comparison (most common):**  
  The model learns which response is better by comparing pairs (chosen vs. rejected).  
- **Pointwise scoring:**  
  Each response is rated independently with a quality score.  
- **Listwise ranking:**  
  Multiple responses are ranked at once (less common, but more expressive).  

Pairwise comparison is preferred because humans find it easier to say *“A is better than B”* rather than giving absolute numeric scores.

---

## Training Workflow
Reward models are typically trained with datasets such as **Dahoas/synthetic-instruction-gptj-pairwise** from Hugging Face, which provide prompts, a preferred response (*chosen*), and a rejected response.  

### Steps
1. **Dataset preparation:**  
    - Prompts are combined with chosen and rejected responses.  
    - Both are tokenized (converted into model-readable format).  
    - Length filtering ensures inputs stay within model limits.  

2. **Scoring function setup:**  
    - A base model (e.g., GPT-2 for sequence classification) is adapted to output a **single scalar score**.  
    - The final layer has **one neuron** that predicts response quality.  

3. **Reward loss (conceptual):**  
    - The model is trained to give higher scores to chosen responses than rejected ones.  
    - Loss functions enforce a larger gap between good and bad scores.  

4. **Evaluation:**  
    - Performance is measured by **win rate**: how often the reward model prefers the correct (chosen) response.  
    - Top models reach 60–70% win rates on real datasets.  

---

## Using LoRA with Reward Models
Reward modeling can be computationally heavy because it requires fine-tuning large models.  
This is where **LoRA (Low-Rank Adaptation)** is useful:

- Instead of updating all model weights, LoRA inserts small trainable matrices into the model.  
- This reduces the number of trainable parameters, lowers memory usage, and speeds up training.  
- LoRA can be applied to the **reward model itself** during training, making preference learning more efficient.  

Example usage:
- Apply LoRA to the classification head and attention layers of the reward model.  
- Configure training with libraries like **PEFT (Parameter-Efficient Fine-Tuning)** and Hugging Face’s `RewardTrainer`.  

---

### Reward Modeling with Other Tuning Techniques
Reward models don’t exist in isolation—they work alongside other fine-tuning methods:

- **Instruction Tuning:**  
  Teaches the model to follow natural language instructions. Reward modeling can refine instruction tuning by ranking which outputs follow instructions *best*.  

- **RLHF (Reinforcement Learning with Human Feedback):**  
  The reward model is used inside RLHF as the “critic” that guides the policy model.  

- **Direct Preference Optimization (DPO):**  
  A newer approach that skips reinforcement learning and directly optimizes on human preferences. Reward modeling concepts still apply here, but training is simplified.  

- **LoRA + Reward Modeling:**  
  Efficiently trains large reward models by reducing compute requirements. This makes RLHF and preference optimization more practical on limited hardware.  

---

### Why Reward Modeling Matters
Reward models are crucial because they:
- Translate **human feedback** into machine-readable signals.  
- Improve **alignment** between LLMs and user intent.  
- Enable efficient use of tuning methods like **LoRA, RLHF, and DPO**.  
- Provide a scalable way to optimize for **factuality, safety, and usefulness**.  

---

### Bradley–Terry Model
The **Bradley–Terry model** is a probabilistic framework used to model pairwise comparisons.  
In the context of reward modeling, it provides a way to assign probabilities that one response is preferred over another.  

- **Use case:** Often applied in pairwise preference training for reward models.  
- **Advantage:** Provides a clear mathematical basis for ranking responses.  
- **Relation:** Forms the foundation for many pairwise reward modeling loss functions.  

---

# Language Models as Distributions and Policies

## LLMs as Distributions
Large Language Models (LLMs) generate text by sampling from a probability distribution over tokens.  
Given a query *X*, the model produces a response *Y* as a sequence of tokens, where each token is drawn from the distribution conditioned on previous tokens.  

- **Token probabilities:** Computed using the softmax function at each timestep.  
- **Sequential dependency:** The distribution at time *t+1* depends on tokens selected at time *t*.  
- **Sampling outcomes:** Multiple valid responses can be generated from the same query, reflecting the stochastic nature of LLMs.  

---

## Generation Parameters
LLMs allow fine control of the sampling process through hyperparameters:  

- **Temperature (τ):** Controls randomness in SoftMax function.  
    - Low τ → deterministic outputs.  
    - High τ → more diverse outputs.  
- **Top-K sampling:** Limits candidate tokens to the *K* highest probabilities.  
- **Top-p sampling (nucleus):** Selects the smallest set of tokens whose cumulative probability exceeds *p*.  
- **Beam search:** Expands and tracks multiple candidate sequences in parallel.  
- **Repetition penalty:** Reduces repeated tokens for more diverse text.  
- **Max/Min tokens:** Constrains output length.  

These parameters balance between accuracy, diversity, and creativity in generation.  

---

## From Distributions to Policies
In reinforcement learning (RL), a **policy** maps states to actions.  
For LLMs, the policy defines how tokens are sampled during generation.  

- **Policy distribution:** Given input *X*, responses *Y* are sampled from the model’s policy.  
- **Rollouts:** Multiple responses generated for the same query; used to explore the policy space.  
- **Exploration vs. exploitation:** Policies introduce randomness to explore diverse text while still favoring likely responses.  

---

## Reinforcement Learning with Human Feedback (RLHF)
RLHF aligns LLMs with human preferences using a reward signal.  

1. **Reward function:** Evaluates query–response pairs with human feedback.  
    - Good answers → high reward.  
    - Poor answers → low reward.  
2. **Rollouts:** Multiple responses per query are sampled and scored.  
3. **Expected reward:** Averaged across responses to guide training.  
4. **Fine-tuning:** Model parameters are updated to maximize expected reward while staying close to the pretrained model.  

This process ensures responses are accurate, safe, and aligned with user intent.  

---

## Proximal Policy Optimization (PPO)
PPO is a reinforcement learning method commonly used in RLHF.  

- **Objective:** Maximize expected reward while preventing drastic changes in the model.  
- **KL penalty:** Keeps the updated policy close to the reference model for stability.  
    - The Kullback-Leibler, or KL, divergence measures the difference between two probability distributions, the desired and the arbitrary policy.
- **Gradient optimization:** Uses sampling tricks (e.g., log-derivative method) to estimate gradients efficiently.  
- **Training tips:**  
    - Regularly evaluate with human feedback.  
    - Start with moderate KL penalty and tune gradually.  
    - Adjust temperature to encourage exploration.  

PPO balances stability and adaptability, making it a practical choice for fine-tuning LLMs.  

---

## Log-Derivative Trick

The **log-derivative trick** (also called the *score function estimator*) is a key technique in machine learning and reinforcement learning.  
It allows us to compute gradients of expectations involving probability distributions, which is essential when optimizing models that generate random outcomes.

### Core Idea
- Instead of directly differentiating through a random sample, we re-express the gradient in terms of the **logarithm of the probability distribution**.  
- This makes it possible to estimate gradients even when the function being optimized is **non-differentiable** with respect to its parameters.  

### Applications
- **Reinforcement Learning (RL):**  
  Used in policy gradient methods where actions are sampled from a probability distribution.  
- **Score Function Estimator:**  
  Provides an unbiased way to estimate gradients by weighting outcomes with their log-probabilities.  
- **Likelihood-based Optimization:**  
  Simplifies gradient computation for likelihood functions, especially in stochastic models.  

### Relation to PPO
The log-derivative trick underlies many **policy optimization algorithms**, including **Proximal Policy Optimization (PPO)**.  
It enables training by relating policy gradients to log-probabilities of actions, making it possible to improve policies efficiently and stably.  

---

## Direct Preference Optimization (DPO)

### Concept
Direct Preference Optimization (DPO) is a reinforcement learning technique that fine-tunes models based directly on human preferences.  
Unlike RLHF with PPO, which requires building and training a reward model, DPO bypasses the need for reinforcement learning loops and directly aligns models with preference data.  

- **Preference collection:** Users compare two or more outputs and select the preferred one.  
- **Efficiency:** Simpler than PPO as it avoids indirect reward maximization.  
- **Reward-free alignment:** Achieves strong results on academic benchmarks without building a reward model.  

---

### Models in DPO
DPO involves three components:  
- **Reward function:** Evaluates relevance or quality of responses.  
- **Target decoder:** Learns to generate aligned responses.  
- **Reference model:** Provides a baseline for comparison and regularization.  

The **objective** is to learn a policy that balances alignment with the reward function while staying close to the reference model, controlled by a regularization parameter (β).  

---

### Partition Function in DPO
The **partition function** is used to normalize probabilities in probability distributions.  

- Ensures the sum of probabilities equals 1.  
- Converts unnormalized functions (e.g., exponential or Gaussian scaling) into valid probability distributions.  
- In DPO, helps reformulate the complex RL objective into a simpler one that avoids explicitly computing the partition function.  

---

### Optimal Solution and KL Divergence
- **Objective functions:** Guide optimization by comparing predictions to targets.  
- **KL divergence:** Measures the difference between the desired policy (π*) and the reference policy (π_ref).  
    - Zero KL divergence means the two distributions are identical.  
- **Transformations:** Maximization can be reformulated as minimization, and scaling or shifting functions does not change the optimum.  
- **Result:** The optimal policy scales the reference model with the reward function, modulated by β.  

---

### Partition Function Complexity
- For sequence length **1:** z(x) sums over all tokens in the vocabulary (|V|).  
- For length **2:** sums over |V|² possible pairs.  
- For length **T:** grows exponentially as |V|ᵀ, making exact computation intractable.  
- DPO avoids this complexity by reformulating the objective to eliminate explicit dependence on z(x).  

---

### Bradley-Terry Model and DPO Loss
- **Pairwise ranking:** Easier for humans than giving absolute scores. Responses are ranked as **Win (W)** or **Loss (L)**.  
- **Bradley-Terry model:** Defines loss as the log of the sigmoid of score differences between W and L.  
- **Integration with DPO:** Reformulates the loss to directly compare policies of W and L, eliminating the need for the partition function.  

**Key insight:**  
The DPO objective becomes a function of the model’s policy and the reference model, removing the need for a separate reward function.  

---

### Training and Implementation
- **Simplification:** Set β = 1 and treat the reference model as a constant baseline.  
- **Loss behavior:**  
  - If policy(W) < policy(L), loss decreases as policy(W) increases.  
  - If policy(W) > policy(L), loss continues to decrease, reinforcing preferred completions.  
- **Cost formulation:** Convert loss into a cost function via negative log-likelihood.  
- **Implementation:**  
  - Custom loss function in PyTorch.  
  - Hugging Face DPO Trainer for practical training.  

---
